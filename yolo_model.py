import keras
from keras.layers import Conv2D,Input,BatchNormalization,\
    LeakyReLU,Add,GlobalAveragePooling2D,Dense,Activation,\
    UpSampling2D,Concatenate,Reshape
from keras.models import Model,load_model
from keras.optimizers import Adam
from keras import backend as K
from keras.preprocessing.image import load_img
from keras.losses import categorical_crossentropy,mean_squared_error
from keras.utils import plot_model
from keras.callbacks import ModelCheckpoint, LearningRateScheduler,TensorBoard,ReduceLROnPlateau

import tensorflow as tf
import numpy as np
from utils import preprocess,softmax

class Basic_Conv():
    def __init__(self,filters,kernel_size,strides=(1,1),batch_normalize=True,
                 pad='same',activation='leaky'):
        self.filters = filters
        self.kernel_size = kernel_size
        self.strides = strides
        self.batch_normalize = batch_normalize
        self.pad = pad
        self.activation = activation
    def __call__(self, food):
        shit = Conv2D(filters=self.filters,
                      kernel_size=self.kernel_size,
                      strides=self.strides,padding=self.pad,use_bias=False)(food)
        shit = BatchNormalization()(shit)# è¿™é‡Œå°†BNæ”¾åœ¨äº†æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚å’ŒåŸä½œè€…ä»£ç ä¿æŒä¸€è‡´
        if self.activation == 'leaky':
            shit = LeakyReLU(alpha=0.1)(shit)
        elif self.activation == 'linear':
            shit = Activation(self.activation)(shit)
        # shit = BatchNormalization()(shit)# è¿™é‡Œå°†BNæ”¾åœ¨äº†æ¿€æ´»å‡½æ•°ä¹‹åã€‚

        return shit
class Basic_Res():# same to [shortcut] in yolov3.cfg
    def __init__(self,activation='linear'):
        self.activation = activation
    def __call__(self, shitA,shitB):
        shit = Add()([shitA,shitB])
        if self.activation == "linear":
            shit = Activation(self.activation)(shit)
        return shit
class Basic_Route():# same to [route] in yolov3.cfg
    def __call__(self, shitA,shitB=None):
        if shitB == None:
            return shitA
        if shitB != None:
            shit = Concatenate()([shitA, shitB])
            return shit

class Basic_Detection():
    # TODO:å¾…æ·»åŠ çœŸæ­£çš„æ£€æµ‹ä»£ç 
    def __call__(self, shit):
        # shit = np.reshape(shit,newshape=(shit.shape[0], shit.shape[1], shit.shape[2], 3, int(shit.shape[3] / 3)))
        shit = Reshape((shit.shape.dims[1].value, shit.shape.dims[2].value, 3, int(shit.shape.dims[3].value / 3)))(shit)
        return shit

class YOLO_V3():
    def __init__(self,config):
        self.config = config
        self.shits = []
        self.inputs = Input(shape=(self.config['model']['image_size'][0], self.config['model']['image_size'][1], 3))
        self.num_classes = len(self.config['model']['classes'])

        # yoloç®—æ³•é‡‡ç”¨å‰åç«¯åˆ†ç¦»ã€‚åç«¯æŒ‡çš„æ˜¯ä¸»å¹²ç½‘ç»œã€‚ä¸»å¹²ç½‘ç»œé…åˆä¸åŒçš„å‰ç«¯ï¼Œå¯ä»¥å®ç°åˆ†ç±»æˆ–è€…æ£€æµ‹çš„ç›®çš„ã€‚
        self.construct_backbone(self.inputs)
        # è½½å…¥é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼ˆä»…ä¸»å¹²ç½‘ç»œï¼‰
        if self.config['model']['type'] == "classification":
            self.construct_classification_model()
        elif self.config['model']['type'] == "detection":
            self.construct_detection_model()
        self.load_pretrain_weights()

        # official_backbone = load_model('weights/darknet53.h5',compile=False)
        # official_backbone.summary(positions=[.33, .6, .7, 1])
        # a=0
        # exit()

    def construct_backbone(self,inputs):
        # è¯¥ä¸»å¹²ç½‘ç»œå’Œyolo v3è®ºæ–‡ä¸ŠèŠ±çš„é‚£ä¸ªå›¾ä¸€æ¨¡ä¸€æ ·ã€‚ä¸åŒ…æ‹¬æœ€åä¸‰å±‚ï¼Œé‚£ä¸‰å±‚æ”¾åˆ°äº†å‰ç«¯é‡Œé¢
        self.shits.append(Basic_Conv(filters=32, kernel_size=3)(inputs))

        self.shits.append(Basic_Conv(filters=64, kernel_size=3, strides=(2, 2))(self.shits[-1]))

        self.shits.append(Basic_Conv(filters=32, kernel_size=1)(self.shits[-1]))
        self.shits.append(Basic_Conv(filters=64, kernel_size=3)(self.shits[-1]))
        self.shits.append(Basic_Res()(self.shits[-1], self.shits[-3]))

        self.shits.append(Basic_Conv(filters=128, kernel_size=3, strides=2)(self.shits[-1]))

        for i in range(2):
            self.shits.append(Basic_Conv(filters=64, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=128, kernel_size=3)(self.shits[-1]))
            self.shits.append(Basic_Res()(self.shits[-1], self.shits[-3]))

        # yolov3.cfg 113~283
        self.shits.append(Basic_Conv(filters=256, kernel_size=3, strides=2)(self.shits[-1]))
        for i in range(8):
            self.shits.append(Basic_Conv(filters=128, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=256, kernel_size=3)(self.shits[-1]))
            self.shits.append(Basic_Res()(self.shits[-1], self.shits[-3]))
        # yolov3.cfg 284~458
        self.shits.append(Basic_Conv(filters=512, kernel_size=3, strides=2)(self.shits[-1]))
        for i in range(8):
            self.shits.append(Basic_Conv(filters=256, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=512, kernel_size=3)(self.shits[-1]))
            self.shits.append(Basic_Res()(self.shits[-1], self.shits[-3]))
        # yolov3.cfg 459~547
        self.shits.append(Basic_Conv(filters=1024, kernel_size=3, strides=2)(self.shits[-1]))
        for i in range(4):
            self.shits.append(Basic_Conv(filters=512, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=1024, kernel_size=3)(self.shits[-1]))
            self.shits.append(Basic_Res()(self.shits[-1], self.shits[-3]))
        #
        self.backbone = Model(inputs,self.shits[-1])
    def construct_classification_model(self):
        self.shits.append(GlobalAveragePooling2D()(self.shits[-1]))
        output_units = self.config['model']['output_units']
        logits = Dense(units=output_units)(self.shits[-1])
        self.model = Model(inputs=self.inputs,outputs=logits)
        self.model.summary()

        plot_model(self.model)
        print("åˆ†ç±»ç½‘ç»œç»„å»ºå®Œæ¯•")
    def construct_detection_model(self):

        for i in range(3):
            self.shits.append(Basic_Conv(filters=512,kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=1024,kernel_size=3)(self.shits[-1]))
        self.shits.append(Basic_Conv(filters=3*(4+1+self.num_classes),kernel_size=1,activation='linear')(self.shits[-1]))
        self.shits.append(Basic_Detection()(self.shits[-1]))#82 First Detection layer, anchors should be large(3 anchors)
        ##################################################
        self.shits.append(Basic_Route()(self.shits[-4]))
        self.shits.append(Basic_Conv(filters=256,kernel_size=1)(self.shits[-1]))
        self.shits.append(UpSampling2D()(self.shits[-1]))
        self.shits.append(Basic_Route()(self.shits[-1],self.shits[61]))
        for i in range(3):
            self.shits.append(Basic_Conv(filters=256, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=512, kernel_size=3)(self.shits[-1]))
        self.shits.append(Basic_Conv(filters=3*(4+1+self.num_classes),kernel_size=1,activation='linear')(self.shits[-1]))
        self.shits.append(Basic_Detection()(self.shits[-1]))#94 Second Detection layer, anchors should be medium(3 anchors)
        ##################################################
        self.shits.append(Basic_Route()(self.shits[-4]))
        self.shits.append(Basic_Conv(filters=128,kernel_size=1)(self.shits[-1]))
        self.shits.append(UpSampling2D()(self.shits[-1]))#out 52*52*128
        self.shits.append(Basic_Route()(self.shits[-1],self.shits[36]))
        for i in range(3):
            self.shits.append(Basic_Conv(filters=128, kernel_size=1)(self.shits[-1]))
            self.shits.append(Basic_Conv(filters=256, kernel_size=3)(self.shits[-1]))
        self.shits.append(Basic_Conv(filters=3*(4+1+self.num_classes),kernel_size=1,activation='linear')(self.shits[-1]))
        self.shits.append(Basic_Detection()(self.shits[-1]))#106 Third Detection layer, anchors should be small(3 anchors)
        ##################################################
        self.model = Model(inputs=self.inputs,outputs=[self.shits[82],self.shits[94],self.shits[106]])

        self.model.summary()
        plot_model(self.model)
        print("ç›®æ ‡æ£€æµ‹ç½‘ç»œæ„å»ºå®Œæ¯•")

    def train(self,train_generator,val_generator):
        config = self.config
        loss_func = tf.losses.softmax_cross_entropy
        checkpoint = ModelCheckpoint(filepath='./tmp/fl_ckpt.h5', monitor='val_acc',
                                     verbose=1, save_best_only=False)

        def lr_sch(epoch):
            # 200 total
            if epoch < 50:
                return 1e-3
            if 50 <= epoch < 100:
                return 1e-4
            if epoch >= 100:
                return 1e-5

        lr_scheduler = LearningRateScheduler(lr_sch)
        lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,
                                       mode='max', min_lr=1e-3)
        tb = TensorBoard(log_dir='./logs',write_graph=False)
        callbacks = [checkpoint, lr_scheduler, lr_reducer,tb]
        self.model.compile(optimizer=Adam(),loss=loss_func,metrics=['accuracy'])
        self.model.fit_generator(generator=train_generator,
                                 steps_per_epoch=800/8,
                                 epochs=config['train']['epochs'],
                                 validation_data=val_generator,
                                 validation_steps=280/8,class_weight='auto',
                                 callbacks=callbacks)
        self.model.save_weights('fl_model.h5')
    def train_detection(self,train_generator,val_generator):
        self.model.compile(optimizer=Adam(),loss=[self.yolo_loss,self.yolo_loss,self.yolo_loss])
        self.model.fit_generator(generator=train_generator,epochs=self.config['train']['epochs'])
    def load_pretrain_weights(self):
        # åŠ è½½é¢„è®­ç»ƒå‚æ•°ã€‚é¦–å…ˆåŠ è½½å®Œå…¨æ¨¡å‹çš„å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰å†åŠ è½½ä¸»å¹²ç½‘ç»œçš„å‚æ•°ã€‚
        if self.config['model']['pretrain_full'] != "":
            self.model.load_weights(self.config['model']['pretrain_full'])
        elif self.config['model']['pretrain_backbone'] != "":
            self.backbone.load_weights(self.config['model']['pretrain_backbone'],by_name=True)
        else:
            print('ï¼ï¼æœªåŠ è½½é¢„è®­ç»ƒå‚æ•°')
    def yolo_loss(self,y_true,y_pred):
        return mean_squared_error(y_true,y_pred)
        # return np.array([[1]])
    def load_weights(self,path):
        self.model.load_weights(path)
    def evaluate(self,generator):
        return self.model.evaluate_generator(generator)
    def predict(self,image_path,class_indices,threshold=0.5):
        import random,os,cv2
        true_class = str(random.randint(0,16))
        target_folder = os.path.join(image_path,true_class)
        files = os.listdir(target_folder)
        fileindex = random.randint(0,len(files)-1)
        target_path = os.path.join(target_folder,files[fileindex])
        image = load_img(target_path,target_size=(256,256))

        np_image = np.array(image)
        image_for_cv2_show = np_image[:, :, ::-1].copy()

        image = np.expand_dims(np_image,axis=0)
        img = preprocess(image)
        result = self.model.predict(img)
        r = softmax(result)
        cv2.imshow('result', image_for_cv2_show)
        true_index = class_indices[true_class]# get one classes's index
        index_classe = dict(zip(class_indices.values(), class_indices.keys()))

        if np.max(r) > 0.5:
            pred_index = np.argmax(r)
            if pred_index == true_index:
                message = " ğŸº"
            else:
                message = " ğŸ’€"
            print('çœŸå€¼ç§ç±»åç§°ï¼š',true_class,'é¢„æµ‹ç±»åˆ«åç§°ï¼š',index_classe[pred_index],message)
        else:
            print('çœŸå€¼ï¼š',true_class,'é¢„æµ‹å€¼ï¼šæ— '," ğŸ’€")
        cv2.waitKey(0)

def prepare_data(train_folder,val_folder):
    from keras.preprocessing.image import ImageDataGenerator
    train_datagen = ImageDataGenerator(rotation_range=30,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       vertical_flip=True,
                                       preprocessing_function=lambda x:((x/255)-0.5)*2)
    val_datagen = ImageDataGenerator(rotation_range=30,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       vertical_flip=True,
                                       preprocessing_function=lambda x:((x/255)-0.5)*2)
    # é»˜è®¤çš„color_modeæ˜¯RGB
    train_generator = train_datagen.flow_from_directory(directory=train_folder,
                                                        target_size=(256,256),batch_size=8)
    val_generator   = val_datagen.flow_from_directory(directory=val_folder,
                                                      target_size=(256,256),batch_size=8)
    return train_generator,val_generator
if __name__ == "__main__":
    pass